# RLlib GFootball â€“ Progressive Self-Play mit Mamba-Hybrid-Netz

[![Python](https://img.shields.io/badge/python-3.10%2B-blue)](https://www.python.org/)
[![PyTorch](https://img.shields.io/badge/PyTorch-2.x-red)](https://pytorch.org/)
[![Ray RLlib](https://img.shields.io/badge/Ray-RLlib-5b5ce2)](https://docs.ray.io/en/latest/rllib/index.html)
[![Google Research Football](https://img.shields.io/badge/Env-GFootball-0a9d57)](https://github.com/google-research/football)
[![License: MIT](https://img.shields.io/badge/License-MIT-green.svg)](#license)

> **Kurzfassung:** Dieses Repo trainiert Google Research Football Agents mit RLlib/IMPALA, einem **Mamba+Attention Hybrid-Netz**, **Trueskill-basiertem Self-Play**, **Champion-Tracking** und **Auto-Pruning** der Policy-Historie. ZusÃ¤tzlich gibtâ€™s einen **Video-Recorder**, der Episoden in â€pixelsâ€œ rendert â€“ perfekt fÃ¼r Demos und PrÃ¤sentationen. âš½ï¸ğŸ§ 

---

## âœ¨ Features

- **Progressives Curriculum** Ã¼ber mehrere Stages (von *Empty Goal* bis *11v11 stochastic*).
- **Self-Play Engine** mit:
  - Trueskill-Ratings (1v1), konservativer Skill (Î¼âˆ’3Ïƒ)
  - **Champion-Tracking** & **geschÃ¼tzte Versionen**
  - **Aktive Zone** + **Top-N** + **Auto-Pruning** alter Gewichte
  - **Gegner-Sampling** nach Match-Quality (+Champion-Bias, Exploration)
- **Population Based Training (PBT)** fÃ¼r Hyperparameter (LR, Entropy, VF-Loss).
- **Mamba-Hybrid-Modell** (Mamba-BlÃ¶cke + Multi-Head Attention) und **Lite-Variante**.
- **Multi-Agent-Wrapper** fÃ¼r GFootball (Links/Rechts-Teams, flexible Spieleranzahl).
- **Video-Demos**: Trained vs. Random-Policy, direkt als MP4/Full Dumps.

---

## ğŸ“¦ Projektstruktur

```
.
â”œâ”€ train.py                 # Hauptskript: Curriculum, Self-Play, PBT, PolicyPool
â”œâ”€ model.py                 # GFootballMambaHybrid2025 & GFootballMambaLite2025
â”œâ”€ demo_record.py           # DualEnvironmentRecorder + Video-Demos
â”œâ”€ requirements.txt         # (Empfohlen) â€“ siehe Installation
â””â”€ README.md                # Diese Datei
```

> Falls deine Dateinamen abweichen: einfach oben anpassen. Die Klasse-/Funktionsnamen sind identisch.

---

## ğŸš€ Installation

> **Voraussetzungen:** Python 3.10+, CUDA optional, FFmpeg (fÃ¼r Video-Export empfohlen)

**1) Umgebung anlegen**
```bash
conda create -n gfootball-rllib python=3.10 -y
conda activate gfootball-rllib
```

**2) AbhÃ¤ngigkeiten**
```bash
# PyTorch (wÃ¤hle passende CUDA-Variante unter https://pytorch.org/get-started/locally/)
pip install torch torchvision --index-url https://download.pytorch.org/whl/cu121

# Ray & RLlib + Extras
pip install "ray[default]" "ray[rllib]" "ray[tune]" "ray[air]"

# Google Research Football
pip install gfootball  # nutzt vorgebaute wheels; bei Problemen siehe GFootball-Doku

# Sonstiges
pip install gymnasium numpy trueskill dataclasses-json
```

> **Windows-Hinweis:** Wenn GFootball-Wheels zicken, nutze WSL2/Ubuntu oder Docker; alternativ vorgebaute Wheels aus Community-Repos.

---

## ğŸ Schnellstart

### 1) Training progressiv starten

In `main()` werden Standard-Umgebungsvariablen gelesen:

- `GFOOTBALL_DEBUG=true|false` â€“ Render/Video, kurze Rollouts, lokaler Modus
- `GFOOTBALL_TRANSFER=true|false` â€“ Transfer Ã¼ber Stages
- `GFOOTBALL_END_STAGE` â€“ Letzte Stage (Index)

**Beispiel (Linux/macOS):**
```bash
export GFOOTBALL_DEBUG=false
export GFOOTBALL_TRANSFER=true
# optional: export GFOOTBALL_END_STAGE=4
python train.py
```

**Beispiel (Windows PowerShell):**
```powershell
$env:GFOOTBALL_DEBUG="false"
$env:GFOOTBALL_TRANSFER="true"
# optional: $env:GFOOTBALL_END_STAGE="4"
python .	rain.py
```

> In `main()` ist ein **Beispiel-Checkpoint** hinterlegt (`initial_checkpoint`). Passe den Pfad an oder setze ihn auf `None`, um von Scratch zu starten.

### 2) TensorBoard

Nach jedem Stage-Run wird ein TensorBoard-Kommando in `training_results_transfer_pbt/tensorboard_commands.txt` ergÃ¤nzt:
```bash
tensorboard --logdir "training_results_transfer_pbt/<experiment>"
```

---

## ğŸ“š Trainings-Curriculum

| Stage | Name                   | Env                                | Left | Right | Target R | Max Steps    | Beschreibung                  |
|:-----:|------------------------|------------------------------------|:----:|:-----:|:--------:|-------------:|------------------------------|
| 1     | stage_1_basic_0        | `academy_empty_goal_close`         | 1    | 0     | 0.75     | 10,000,000   | 1 Spieler vor Tor            |
| 2     | stage_1_basic_1        | `academy_run_to_score_with_keeper` | 1    | 0     | 0.75     | 10,000,000   | 1 Spieler rennt zum Tor      |
| 3     | stage_1_basic          | `academy_pass_and_shoot_with_keeper` | 1  | 0     | 0.75     | 10,000,000   | 1 Spieler gegen Keeper       |
| 4     | stage_2_1v1            | `academy_3_vs_1_with_keeper`       | 3    | 0     | 0.75     | 20,000,000   | 1v1 (3 vs 1 Setup)           |
| 5     | stage_3_3v3            | `11_vs_11_easy_stochastic`         | 3    | 0     | 1.0      | 50,000,000   | 3v3 (easy)                   |
| 6     | stage_4_3v3            | `11_vs_11_easy_stochastic`         | 3    | 3     | 1.0      | 100,000,000  | 3v3 (easy, beide Seiten)     |
| 7     | stage_5_3v3            | `11_vs_11_stochastic`              | 3    | 3     | 1.0      | 500,000,000  | 3v3 (stochastic, schwerer)   |

> **Hinweis:** Die Stop-Kriterien nutzen `timesteps_total`. Das `target_reward` ist als Orientierung gedacht und wird (je nach Bedarf) nicht hart erzwungen.

---

## ğŸ§  Architektur

### Mamba-Hybrid-Netz
- **MambaBlock:** depthwise Conv-Vorverarbeitung â†’ vereinfachte SSM-Dynamik (A/B mean pooling) â†’ Gate â†’ Output-Proj.
- **MultiHeadSpatialAttention** (optional in jedem zweiten Block)
- **Feed-Forward (Mish)** + Residuals/LayerNorm
- **Head-Design:** getrennte Policy/Value-Heads, orthogonale Init (NormC)

### IMPALA + RLlib
- **Framework:** PyTorch AMP (optional), Grad-Clip, groÃŸe `train_batch_size`
- **PBT:** Perturbation alle 64k Steps, Mutationen auf LR/Entropy/VF-Coeff
- **Multi-Agent:** Left/Right Policies, dynamische `policies_to_train`

---

## ğŸ¥Š Self-Play & PolicyPool

- **Versioning:** Neue Versionen werden periodisch gespeichert (`version_save_interval`).
- **Trueskill-Update:** Aggregiert Episoden-Scores; Sieg/Unentschieden/Niederlage â†’ `rate_1vs1`.
- **Champion-Schutz:** Beste konservative Skill-Version wird nie gepruned.
- **Active Set:** `keep_top_n` (nach Skill), jÃ¼ngste `active_zone_size`, aktuelle Version.
- **Auto-Pruning:** LÃ¶scht Gewichte & Ratings alter Versionen (auÃŸer Schutz/Active).

Konfigurierbar in `EnhancedSelfPlayCallback(...)` sowie `PolicyPool(...)`.

---

## ğŸ¬ Video-Demos

`demo_record.py` rendert Episoden in â€pixelsâ€œ parallel zur Trainingsumgebung (â€simple115v2â€œ).  
Du kannst **trainierte Checkpoints** oder **Random-Policies** aufnehmen.

**Beispiel:**
```python
USE_RANDOM_WEIGHTS = False
CHECKPOINT_PFAD = r"C:\clones\rlib_gfootball\training_results_transfer_pbt\stage_1_basic_1_20251019_191114\9e3b0_00000\checkpoint_000095"
STAGE_KEY = "s3_pass_shoot"   # siehe DEMO_STAGES
NUM_EPISODEN = 5

create_video_demo(CHECKPOINT_PFAD, STAGE_KEY, NUM_EPISODEN)
```

Ausgabe landet unter `presentation_demo_<stage>_trained_<timestamp>/videos/`.

---

## âš™ï¸ NÃ¼tzliche Umgebungsvariablen

```bash
# Debug/Render kurz halten, lokaler Modus, 0 Runner
export GFOOTBALL_DEBUG=true

# Transfer Learning Ã¼ber die Curriculum-Stages
export GFOOTBALL_TRANSFER=true

# Optional: letzte Stage als Index (0-basiert)
export GFOOTBALL_END_STAGE=4
```

---

## ğŸ§ª Troubleshooting

- **GFootball hÃ¤ngt/kein Render:** Stelle sicher, dass FFmpeg installiert ist; ggf. `write_video=False` setzen.
- **Windows Pfade:** Backslashes escapen oder `r"raw\strings"` nutzen (siehe Beispiele).
- **CUDA/AMP Fehler:** Setze `use_amp=False` im `custom_model_config`.
- **â€Cant call step() once episode finishedâ€œ beim Video:** Wird intern abgefangen; tritt bei sehr kurzen Episoden auf.
- **Speicherverbrauch hoch:** `train_batch_size` reduzieren, `num_env_runners`/`num_envs_per_env_runner` anpassen.

---

## ğŸ“ˆ Konfiguration anpassen (Beispiel)

```python
config = ImpalaConfig()   .environment("gfootball_multi", env_config={
      "env_name": "11_vs_11_easy_stochastic",
      "representation": "simple115v2",
      "number_of_left_players_agent_controls": 3,
      "number_of_right_players_agent_controls": 3,
      "rewards": "scoring,checkpoints",
      "stacked": True,
  })   .framework("torch")   .env_runners(num_env_runners=5, num_envs_per_env_runner=1)   .training(
      lr=1e-4, entropy_coeff=0.008, vf_loss_coeff=0.5,
      grad_clip=0.5, train_batch_size=4096,
      model={
        "custom_model": "mamba_hybrid_2025",
        "custom_model_config": {
          "d_model": 256, "num_layers": 4, "d_state": 16,
          "num_heads": 4, "use_attention": True,
          "dropout": 0.1, "use_amp": True
        }
      },
  )
```

---

## ğŸ¤ Danksagung

- [Google Research Football](https://github.com/google-research/football)
- [Ray RLlib](https://docs.ray.io/en/latest/rllib/index.html)
- Arbeiten rund um **Mamba/SSM** als Inspiration fÃ¼r die Hybrid-BlÃ¶cke.

---

## ğŸ“ License

Dieses Projekt steht unter der **MIT-Lizenz**. Siehe [LICENSE](LICENSE) fÃ¼r Details.

---

## ğŸ“« Kontakt

Issues & PRs sind willkommen!  
Wenn du Benchmarks, Logs oder Videos teilen magst: gerne verlinken ğŸ‘‡

- TensorBoard: siehe `training_results_transfer_pbt/tensorboard_commands.txt`
- Videos: Ordner `presentation_demo_*/videos/`

---

Viel SpaÃŸ beim Trainieren & Kicken! âš½ï¸ğŸ”¥
