# RLlib GFootball ‚Äì Progressive Self-Play mit Mamba-Hybrid-Netz

[![Python](https://img.shields.io/badge/python-3.10%2B-blue)](https://www.python.org/)
[![PyTorch](https://img.shields.io/badge/PyTorch-2.x-red)](https://pytorch.org/)
[![Ray RLlib](https://img.shields.io/badge/Ray-RLlib-5b5ce2)](https://docs.ray.io/en/latest/rllib/index.html)
[![Google Research Football](https://img.shields.io/badge/Env-GFootball-0a9d57)](https://github.com/google-research/football)
[![License: MIT](https://img.shields.io/badge/License-MIT-green.svg)](#license)

> **Kurzfassung:** Dieses Repo trainiert Google Research Football Agents mit RLlib/IMPALA, einem **Mamba+Attention Hybrid-Netz**, **Trueskill-basiertem Self-Play**, **Champion-Tracking** und **Auto-Pruning** der Policy-Historie. Zus√§tzlich gibt‚Äôs einen **Video-Recorder**, der Episoden in ‚Äûpixels‚Äú rendert ‚Äì perfekt f√ºr Demos und Pr√§sentationen. ‚öΩÔ∏èüß†

---

## ‚ú® Features

- **Progressives Curriculum** √ºber mehrere Stages (von *Empty Goal* bis *11v11 stochastic*).
- **Self-Play Engine** mit:
  - Trueskill-Ratings (1v1), konservativer Skill (Œº‚àí3œÉ)
  - **Champion-Tracking** & **gesch√ºtzte Versionen**
  - **Aktive Zone** + **Top-N** + **Auto-Pruning** alter Gewichte
  - **Gegner-Sampling** nach Match-Quality (+Champion-Bias, Exploration)
- **Population Based Training (PBT)** f√ºr Hyperparameter (LR, Entropy, VF-Loss).
- **Mamba-Hybrid-Modell** (Mamba-Bl√∂cke + Multi-Head Attention) und **Lite-Variante**.
- **Multi-Agent-Wrapper** f√ºr GFootball (Links/Rechts-Teams, flexible Spieleranzahl).
- **Video-Demos**: Trained vs. Random-Policy, direkt als MP4/Full Dumps.

---

## üì¶ Projektstruktur

```
.
‚îú‚îÄ train.py                 # Hauptskript: Curriculum, Self-Play, PBT, PolicyPool
‚îú‚îÄ model.py                 # GFootballMambaHybrid2025 & GFootballMambaLite2025
‚îú‚îÄ demo_record.py           # DualEnvironmentRecorder + Video-Demos
‚îú‚îÄ requirements.txt         # (Empfohlen) ‚Äì siehe Installation
‚îî‚îÄ README.md                # Diese Datei
```

> Falls deine Dateinamen abweichen: einfach oben anpassen. Die Klasse-/Funktionsnamen sind identisch.

---

## üöÄ Installation

> **Voraussetzungen:** Python 3.10+, CUDA optional, FFmpeg (f√ºr Video-Export empfohlen)

**1) Umgebung anlegen**
```bash
conda create -n gfootball-rllib python=3.10 -y
conda activate gfootball-rllib
```

**2) Abh√§ngigkeiten**
```bash
# PyTorch (w√§hle passende CUDA-Variante unter https://pytorch.org/get-started/locally/)
pip install torch torchvision --index-url https://download.pytorch.org/whl/cu121

# Ray & RLlib + Extras
pip install "ray[default]" "ray[rllib]" "ray[tune]" "ray[air]"

# Google Research Football
pip install gfootball  # nutzt vorgebaute wheels; bei Problemen siehe GFootball-Doku

# Sonstiges
pip install gymnasium numpy trueskill dataclasses-json
```

> **Windows-Hinweis:** Wenn GFootball-Wheels zicken, nutze WSL2/Ubuntu oder Docker; alternativ vorgebaute Wheels aus Community-Repos.

---

## üèÅ Schnellstart

### 1) Training progressiv starten

In `main()` werden Standard-Umgebungsvariablen gelesen:

- `GFOOTBALL_DEBUG=true|false` ‚Äì Render/Video, kurze Rollouts, lokaler Modus
- `GFOOTBALL_TRANSFER=true|false` ‚Äì Transfer √ºber Stages
- `GFOOTBALL_END_STAGE` ‚Äì Letzte Stage (Index)

**Beispiel (Linux/macOS):**
```bash
export GFOOTBALL_DEBUG=false
export GFOOTBALL_TRANSFER=true
# optional: export GFOOTBALL_END_STAGE=4
python train.py
```

**Beispiel (Windows PowerShell):**
```powershell
$env:GFOOTBALL_DEBUG="false"
$env:GFOOTBALL_TRANSFER="true"
# optional: $env:GFOOTBALL_END_STAGE="4"
python .	rain.py
```

> In `main()` ist ein **Beispiel-Checkpoint** hinterlegt (`initial_checkpoint`). Passe den Pfad an oder setze ihn auf `None`, um von Scratch zu starten.

### 2) TensorBoard

Nach jedem Stage-Run wird ein TensorBoard-Kommando in `training_results_transfer_pbt/tensorboard_commands.txt` erg√§nzt:
```bash
tensorboard --logdir "training_results_transfer_pbt/<experiment>"
```

---

## üìö Trainings-Curriculum

| Stage | Name                   | Env                                | Left | Right | Target R | Max Steps    | Beschreibung                  |
|:-----:|------------------------|------------------------------------|:----:|:-----:|:--------:|-------------:|------------------------------|
| 1     | stage_1_basic_0        | `academy_empty_goal_close`         | 1    | 0     | 0.75     | 10,000,000   | 1 Spieler vor Tor            |
| 2     | stage_1_basic_1        | `academy_run_to_score_with_keeper` | 1    | 0     | 0.75     | 10,000,000   | 1 Spieler rennt zum Tor      |
| 3     | stage_1_basic          | `academy_pass_and_shoot_with_keeper` | 1  | 0     | 0.75     | 10,000,000   | 1 Spieler gegen Keeper       |
| 4     | stage_2_1v1            | `academy_3_vs_1_with_keeper`       | 3    | 0     | 0.75     | 20,000,000   | 1v1 (3 vs 1 Setup)           |
| 5     | stage_3_3v3            | `11_vs_11_easy_stochastic`         | 3    | 0     | 1.0      | 50,000,000   | 3v3 (easy)                   |
| 6     | stage_4_3v3            | `11_vs_11_easy_stochastic`         | 3    | 3     | 1.0      | 100,000,000  | 3v3 (easy, beide Seiten)     |
| 7     | stage_5_3v3            | `11_vs_11_stochastic`              | 3    | 3     | 1.0      | 500,000,000  | 3v3 (stochastic, schwerer)   |

> **Hinweis:** Die Stop-Kriterien nutzen `timesteps_total`. Das `target_reward` ist als Orientierung gedacht und wird (je nach Bedarf) nicht hart erzwungen.

---

## üß† Architektur

### Mamba-Hybrid-Netz
- **MambaBlock:** depthwise Conv-Vorverarbeitung ‚Üí vereinfachte SSM-Dynamik (A/B mean pooling) ‚Üí Gate ‚Üí Output-Proj.
- **MultiHeadSpatialAttention** (optional in jedem zweiten Block)
- **Feed-Forward (Mish)** + Residuals/LayerNorm
- **Head-Design:** getrennte Policy/Value-Heads, orthogonale Init (NormC)

### IMPALA + RLlib
- **Framework:** PyTorch AMP (optional), Grad-Clip, gro√üe `train_batch_size`
- **PBT:** Perturbation alle 64k Steps, Mutationen auf LR/Entropy/VF-Coeff
- **Multi-Agent:** Left/Right Policies, dynamische `policies_to_train`

---

## ü•ä Self-Play & PolicyPool

- **Versioning:** Neue Versionen werden periodisch gespeichert (`version_save_interval`).
- **Trueskill-Update:** Aggregiert Episoden-Scores; Sieg/Unentschieden/Niederlage ‚Üí `rate_1vs1`.
- **Champion-Schutz:** Beste konservative Skill-Version wird nie gepruned.
- **Active Set:** `keep_top_n` (nach Skill), j√ºngste `active_zone_size`, aktuelle Version.
- **Auto-Pruning:** L√∂scht Gewichte & Ratings alter Versionen (au√üer Schutz/Active).

Konfigurierbar in `EnhancedSelfPlayCallback(...)` sowie `PolicyPool(...)`.

---

## üé¨ Video-Demos

`demo_record.py` rendert Episoden in ‚Äûpixels‚Äú parallel zur Trainingsumgebung (‚Äûsimple115v2‚Äú).  
Du kannst **trainierte Checkpoints** oder **Random-Policies** aufnehmen.

**Beispiel:**
```python
USE_RANDOM_WEIGHTS = False
CHECKPOINT_PFAD = r"C:\clones\rlib_gfootball\training_results_transfer_pbt\stage_1_basic_1_20251019_191114\9e3b0_00000\checkpoint_000095"
STAGE_KEY = "s3_pass_shoot"   # siehe DEMO_STAGES
NUM_EPISODEN = 5

create_video_demo(CHECKPOINT_PFAD, STAGE_KEY, NUM_EPISODEN)
```

Ausgabe landet unter `presentation_demo_<stage>_trained_<timestamp>/videos/`.

---

## ‚öôÔ∏è N√ºtzliche Umgebungsvariablen

```bash
# Debug/Render kurz halten, lokaler Modus, 0 Runner
export GFOOTBALL_DEBUG=true

# Transfer Learning √ºber die Curriculum-Stages
export GFOOTBALL_TRANSFER=true

# Optional: letzte Stage als Index (0-basiert)
export GFOOTBALL_END_STAGE=4
```

---

## üß™ Troubleshooting

- **GFootball h√§ngt/kein Render:** Stelle sicher, dass FFmpeg installiert ist; ggf. `write_video=False` setzen.
- **Windows Pfade:** Backslashes escapen oder `r"raw\strings"` nutzen (siehe Beispiele).
- **CUDA/AMP Fehler:** Setze `use_amp=False` im `custom_model_config`.
- **‚ÄûCant call step() once episode finished‚Äú beim Video:** Wird intern abgefangen; tritt bei sehr kurzen Episoden auf.
- **Speicherverbrauch hoch:** `train_batch_size` reduzieren, `num_env_runners`/`num_envs_per_env_runner` anpassen.

---

## üìà Konfiguration anpassen (Beispiel)

```python
config = ImpalaConfig()   .environment("gfootball_multi", env_config={
      "env_name": "11_vs_11_easy_stochastic",
      "representation": "simple115v2",
      "number_of_left_players_agent_controls": 3,
      "number_of_right_players_agent_controls": 3,
      "rewards": "scoring,checkpoints",
      "stacked": True,
  })   .framework("torch")   .env_runners(num_env_runners=5, num_envs_per_env_runner=1)   .training(
      lr=1e-4, entropy_coeff=0.008, vf_loss_coeff=0.5,
      grad_clip=0.5, train_batch_size=4096,
      model={
        "custom_model": "mamba_hybrid_2025",
        "custom_model_config": {
          "d_model": 256, "num_layers": 4, "d_state": 16,
          "num_heads": 4, "use_attention": True,
          "dropout": 0.1, "use_amp": True
        }
      },
  )
```

---

## ü§ù Danksagung

- [Google Research Football](https://github.com/google-research/football)
- [Ray RLlib](https://docs.ray.io/en/latest/rllib/index.html)
- Arbeiten rund um **Mamba/SSM** als Inspiration f√ºr die Hybrid-Bl√∂cke.

---

## üìù License

Dieses Projekt steht unter der **MIT-Lizenz**. Siehe [LICENSE](LICENSE) f√ºr Details.

---

## üì´ Kontakt

Issues & PRs sind willkommen!  
Wenn du Benchmarks, Logs oder Videos teilen magst: gerne verlinken üëá

- TensorBoard: siehe `training_results_transfer_pbt/tensorboard_commands.txt`
- Videos: Ordner `presentation_demo_*/videos/`

---

Viel Spa√ü beim Trainieren & Kicken! ‚öΩÔ∏èüî•

MambaPPoStage2
+---------------------------------+----------+-----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name                      | status   | loc             |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|---------------------------------+----------+-----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_gfootball_multi_236a1_00000 | RUNNING  | 127.0.0.1:3848  |   1194 |          53363   | 39684468 |   1.0375 |                    2 |                  0.1 |            67.7459 |
| PPO_gfootball_multi_236a1_00001 | RUNNING  | 127.0.0.1:37040 |   1204 |          53714.2 | 40016984 |   1.1538 |                    2 |                  0.1 |            71.7722 |
+---------------------------------+----------+-----------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+